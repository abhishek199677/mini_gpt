{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"data.txt\", 'r') as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sun rises in the east and sets in the west.\\nCats are known for their agility and independence.\\nArtificial intelligence is transforming many industries.\\nThe quick brown fox jumps over the lazy dog.\\nPython is a popular programming language for data science.\\nHealthy eating and regular exercise are good for your health.\\nSpace exploration has led to many technological advances.\\nLearning new skills can open up many opportunities.\\nThe weather today is sunny with a gentle breeze.\\nBooks can take you on amazing adventures without leaving your home.\\nMusic has the power to lift your spirits and relax your mind.\\nTraveling to new places helps you understand different cultures.\\nTechnology continues to evolve at a rapid pace.\\nCooking at home can be both fun and economical.\\nEnvironmental conservation is essential for our future.\\nOnline education has become more popular in recent years.\\nGardening is a relaxing hobby that also benefits the environment.\\nThe internet has revolutionized the way we communicate.\\nRegular reading improves vocabulary and comprehension.\\nFitness routines can be tailored to suit individual needs.\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizing the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts([text_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('the', 10),\n",
       "             ('sun', 1),\n",
       "             ('rises', 1),\n",
       "             ('in', 3),\n",
       "             ('east', 1),\n",
       "             ('and', 6),\n",
       "             ('sets', 1),\n",
       "             ('west', 1),\n",
       "             ('cats', 1),\n",
       "             ('are', 2),\n",
       "             ('known', 1),\n",
       "             ('for', 4),\n",
       "             ('their', 1),\n",
       "             ('agility', 1),\n",
       "             ('independence', 1),\n",
       "             ('artificial', 1),\n",
       "             ('intelligence', 1),\n",
       "             ('is', 5),\n",
       "             ('transforming', 1),\n",
       "             ('many', 3),\n",
       "             ('industries', 1),\n",
       "             ('quick', 1),\n",
       "             ('brown', 1),\n",
       "             ('fox', 1),\n",
       "             ('jumps', 1),\n",
       "             ('over', 1),\n",
       "             ('lazy', 1),\n",
       "             ('dog', 1),\n",
       "             ('python', 1),\n",
       "             ('a', 4),\n",
       "             ('popular', 2),\n",
       "             ('programming', 1),\n",
       "             ('language', 1),\n",
       "             ('data', 1),\n",
       "             ('science', 1),\n",
       "             ('healthy', 1),\n",
       "             ('eating', 1),\n",
       "             ('regular', 2),\n",
       "             ('exercise', 1),\n",
       "             ('good', 1),\n",
       "             ('your', 4),\n",
       "             ('health', 1),\n",
       "             ('space', 1),\n",
       "             ('exploration', 1),\n",
       "             ('has', 4),\n",
       "             ('led', 1),\n",
       "             ('to', 5),\n",
       "             ('technological', 1),\n",
       "             ('advances', 1),\n",
       "             ('learning', 1),\n",
       "             ('new', 2),\n",
       "             ('skills', 1),\n",
       "             ('can', 4),\n",
       "             ('open', 1),\n",
       "             ('up', 1),\n",
       "             ('opportunities', 1),\n",
       "             ('weather', 1),\n",
       "             ('today', 1),\n",
       "             ('sunny', 1),\n",
       "             ('with', 1),\n",
       "             ('gentle', 1),\n",
       "             ('breeze', 1),\n",
       "             ('books', 1),\n",
       "             ('take', 1),\n",
       "             ('you', 2),\n",
       "             ('on', 1),\n",
       "             ('amazing', 1),\n",
       "             ('adventures', 1),\n",
       "             ('without', 1),\n",
       "             ('leaving', 1),\n",
       "             ('home', 2),\n",
       "             ('music', 1),\n",
       "             ('power', 1),\n",
       "             ('lift', 1),\n",
       "             ('spirits', 1),\n",
       "             ('relax', 1),\n",
       "             ('mind', 1),\n",
       "             ('traveling', 1),\n",
       "             ('places', 1),\n",
       "             ('helps', 1),\n",
       "             ('understand', 1),\n",
       "             ('different', 1),\n",
       "             ('cultures', 1),\n",
       "             ('technology', 1),\n",
       "             ('continues', 1),\n",
       "             ('evolve', 1),\n",
       "             ('at', 2),\n",
       "             ('rapid', 1),\n",
       "             ('pace', 1),\n",
       "             ('cooking', 1),\n",
       "             ('be', 2),\n",
       "             ('both', 1),\n",
       "             ('fun', 1),\n",
       "             ('economical', 1),\n",
       "             ('environmental', 1),\n",
       "             ('conservation', 1),\n",
       "             ('essential', 1),\n",
       "             ('our', 1),\n",
       "             ('future', 1),\n",
       "             ('online', 1),\n",
       "             ('education', 1),\n",
       "             ('become', 1),\n",
       "             ('more', 1),\n",
       "             ('recent', 1),\n",
       "             ('years', 1),\n",
       "             ('gardening', 1),\n",
       "             ('relaxing', 1),\n",
       "             ('hobby', 1),\n",
       "             ('that', 1),\n",
       "             ('also', 1),\n",
       "             ('benefits', 1),\n",
       "             ('environment', 1),\n",
       "             ('internet', 1),\n",
       "             ('revolutionized', 1),\n",
       "             ('way', 1),\n",
       "             ('we', 1),\n",
       "             ('communicate', 1),\n",
       "             ('reading', 1),\n",
       "             ('improves', 1),\n",
       "             ('vocabulary', 1),\n",
       "             ('comprehension', 1),\n",
       "             ('fitness', 1),\n",
       "             ('routines', 1),\n",
       "             ('tailored', 1),\n",
       "             ('suit', 1),\n",
       "             ('individual', 1),\n",
       "             ('needs', 1)])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts  # how many times each word is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'the': 2,\n",
       " 'and': 3,\n",
       " 'is': 4,\n",
       " 'to': 5,\n",
       " 'for': 6,\n",
       " 'a': 7,\n",
       " 'your': 8,\n",
       " 'has': 9,\n",
       " 'can': 10,\n",
       " 'in': 11,\n",
       " 'many': 12,\n",
       " 'are': 13,\n",
       " 'popular': 14,\n",
       " 'regular': 15,\n",
       " 'new': 16,\n",
       " 'you': 17,\n",
       " 'home': 18,\n",
       " 'at': 19,\n",
       " 'be': 20,\n",
       " 'sun': 21,\n",
       " 'rises': 22,\n",
       " 'east': 23,\n",
       " 'sets': 24,\n",
       " 'west': 25,\n",
       " 'cats': 26,\n",
       " 'known': 27,\n",
       " 'their': 28,\n",
       " 'agility': 29,\n",
       " 'independence': 30,\n",
       " 'artificial': 31,\n",
       " 'intelligence': 32,\n",
       " 'transforming': 33,\n",
       " 'industries': 34,\n",
       " 'quick': 35,\n",
       " 'brown': 36,\n",
       " 'fox': 37,\n",
       " 'jumps': 38,\n",
       " 'over': 39,\n",
       " 'lazy': 40,\n",
       " 'dog': 41,\n",
       " 'python': 42,\n",
       " 'programming': 43,\n",
       " 'language': 44,\n",
       " 'data': 45,\n",
       " 'science': 46,\n",
       " 'healthy': 47,\n",
       " 'eating': 48,\n",
       " 'exercise': 49,\n",
       " 'good': 50,\n",
       " 'health': 51,\n",
       " 'space': 52,\n",
       " 'exploration': 53,\n",
       " 'led': 54,\n",
       " 'technological': 55,\n",
       " 'advances': 56,\n",
       " 'learning': 57,\n",
       " 'skills': 58,\n",
       " 'open': 59,\n",
       " 'up': 60,\n",
       " 'opportunities': 61,\n",
       " 'weather': 62,\n",
       " 'today': 63,\n",
       " 'sunny': 64,\n",
       " 'with': 65,\n",
       " 'gentle': 66,\n",
       " 'breeze': 67,\n",
       " 'books': 68,\n",
       " 'take': 69,\n",
       " 'on': 70,\n",
       " 'amazing': 71,\n",
       " 'adventures': 72,\n",
       " 'without': 73,\n",
       " 'leaving': 74,\n",
       " 'music': 75,\n",
       " 'power': 76,\n",
       " 'lift': 77,\n",
       " 'spirits': 78,\n",
       " 'relax': 79,\n",
       " 'mind': 80,\n",
       " 'traveling': 81,\n",
       " 'places': 82,\n",
       " 'helps': 83,\n",
       " 'understand': 84,\n",
       " 'different': 85,\n",
       " 'cultures': 86,\n",
       " 'technology': 87,\n",
       " 'continues': 88,\n",
       " 'evolve': 89,\n",
       " 'rapid': 90,\n",
       " 'pace': 91,\n",
       " 'cooking': 92,\n",
       " 'both': 93,\n",
       " 'fun': 94,\n",
       " 'economical': 95,\n",
       " 'environmental': 96,\n",
       " 'conservation': 97,\n",
       " 'essential': 98,\n",
       " 'our': 99,\n",
       " 'future': 100,\n",
       " 'online': 101,\n",
       " 'education': 102,\n",
       " 'become': 103,\n",
       " 'more': 104,\n",
       " 'recent': 105,\n",
       " 'years': 106,\n",
       " 'gardening': 107,\n",
       " 'relaxing': 108,\n",
       " 'hobby': 109,\n",
       " 'that': 110,\n",
       " 'also': 111,\n",
       " 'benefits': 112,\n",
       " 'environment': 113,\n",
       " 'internet': 114,\n",
       " 'revolutionized': 115,\n",
       " 'way': 116,\n",
       " 'we': 117,\n",
       " 'communicate': 118,\n",
       " 'reading': 119,\n",
       " 'improves': 120,\n",
       " 'vocabulary': 121,\n",
       " 'comprehension': 122,\n",
       " 'fitness': 123,\n",
       " 'routines': 124,\n",
       " 'tailored': 125,\n",
       " 'suit': 126,\n",
       " 'individual': 127,\n",
       " 'needs': 128}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index   # how many times each word is repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['sun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['cats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.num_words   # total number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  21,\n",
       "  22,\n",
       "  11,\n",
       "  2,\n",
       "  23,\n",
       "  3,\n",
       "  24,\n",
       "  11,\n",
       "  2,\n",
       "  25,\n",
       "  26,\n",
       "  13,\n",
       "  27,\n",
       "  6,\n",
       "  28,\n",
       "  29,\n",
       "  3,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  4,\n",
       "  33,\n",
       "  12,\n",
       "  34,\n",
       "  2,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  2,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  4,\n",
       "  7,\n",
       "  14,\n",
       "  43,\n",
       "  44,\n",
       "  6,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  3,\n",
       "  15,\n",
       "  49,\n",
       "  13,\n",
       "  50,\n",
       "  6,\n",
       "  8,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  9,\n",
       "  54,\n",
       "  5,\n",
       "  12,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  16,\n",
       "  58,\n",
       "  10,\n",
       "  59,\n",
       "  60,\n",
       "  12,\n",
       "  61,\n",
       "  2,\n",
       "  62,\n",
       "  63,\n",
       "  4,\n",
       "  64,\n",
       "  65,\n",
       "  7,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  10,\n",
       "  69,\n",
       "  17,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  8,\n",
       "  18,\n",
       "  75,\n",
       "  9,\n",
       "  2,\n",
       "  76,\n",
       "  5,\n",
       "  77,\n",
       "  8,\n",
       "  78,\n",
       "  3,\n",
       "  79,\n",
       "  8,\n",
       "  80,\n",
       "  81,\n",
       "  5,\n",
       "  16,\n",
       "  82,\n",
       "  83,\n",
       "  17,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  5,\n",
       "  89,\n",
       "  19,\n",
       "  7,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  19,\n",
       "  18,\n",
       "  10,\n",
       "  20,\n",
       "  93,\n",
       "  94,\n",
       "  3,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  4,\n",
       "  98,\n",
       "  6,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  9,\n",
       "  103,\n",
       "  104,\n",
       "  14,\n",
       "  11,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  4,\n",
       "  7,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  2,\n",
       "  113,\n",
       "  2,\n",
       "  114,\n",
       "  9,\n",
       "  115,\n",
       "  2,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  15,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  3,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  10,\n",
       "  20,\n",
       "  125,\n",
       "  5,\n",
       "  126,\n",
       "  127,\n",
       "  128]]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([text_data])  # iconverts exact same text into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 21,\n",
       " 22,\n",
       " 11,\n",
       " 2,\n",
       " 23,\n",
       " 3,\n",
       " 24,\n",
       " 11,\n",
       " 2,\n",
       " 25,\n",
       " 26,\n",
       " 13,\n",
       " 27,\n",
       " 6,\n",
       " 28,\n",
       " 29,\n",
       " 3,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 4,\n",
       " 33,\n",
       " 12,\n",
       " 34,\n",
       " 2,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 2,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 4,\n",
       " 7,\n",
       " 14,\n",
       " 43,\n",
       " 44,\n",
       " 6,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 3,\n",
       " 15,\n",
       " 49,\n",
       " 13,\n",
       " 50,\n",
       " 6,\n",
       " 8,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 9,\n",
       " 54,\n",
       " 5,\n",
       " 12,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 16,\n",
       " 58,\n",
       " 10,\n",
       " 59,\n",
       " 60,\n",
       " 12,\n",
       " 61,\n",
       " 2,\n",
       " 62,\n",
       " 63,\n",
       " 4,\n",
       " 64,\n",
       " 65,\n",
       " 7,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 10,\n",
       " 69,\n",
       " 17,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 8,\n",
       " 18,\n",
       " 75,\n",
       " 9,\n",
       " 2,\n",
       " 76,\n",
       " 5,\n",
       " 77,\n",
       " 8,\n",
       " 78,\n",
       " 3,\n",
       " 79,\n",
       " 8,\n",
       " 80,\n",
       " 81,\n",
       " 5,\n",
       " 16,\n",
       " 82,\n",
       " 83,\n",
       " 17,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 5,\n",
       " 89,\n",
       " 19,\n",
       " 7,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 19,\n",
       " 18,\n",
       " 10,\n",
       " 20,\n",
       " 93,\n",
       " 94,\n",
       " 3,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 4,\n",
       " 98,\n",
       " 6,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 9,\n",
       " 103,\n",
       " 104,\n",
       " 14,\n",
       " 11,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 4,\n",
       " 7,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 2,\n",
       " 113,\n",
       " 2,\n",
       " 114,\n",
       " 9,\n",
       " 115,\n",
       " 2,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 15,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 3,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 10,\n",
       " 20,\n",
       " 125,\n",
       " 5,\n",
       " 126,\n",
       " 127,\n",
       " 128]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences([text_data])[0]   # converting text into numbers with the help of tokenizer  \n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence)  # lenght of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### storing tokenizer in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing x and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 100\n",
    "\n",
    "def create_dataset(seq, max_seq_length):\n",
    "    input = []\n",
    "    labels = []\n",
    "    for i in range(0, len(seq) - max_seq_length):\n",
    "        input.append(seq[i: i + max_seq_length])\n",
    "        labels.append(seq[i + 1: i + max_seq_length + 1])\n",
    "    return np.array(input), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = create_dataset(sequence, max_seq_length)  #created x and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,  21,  22, ...,   3,  79,   8],\n",
       "       [ 21,  22,  11, ...,  79,   8,  80],\n",
       "       [ 22,  11,   2, ...,   8,  80,  81],\n",
       "       ...,\n",
       "       [ 64,  65,   7, ...,  20, 125,   5],\n",
       "       [ 65,   7,  66, ..., 125,   5, 126],\n",
       "       [  7,  66,  67, ...,   5, 126, 127]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21,  22,  11, ...,  79,   8,  80],\n",
       "       [ 22,  11,   2, ...,   8,  80,  81],\n",
       "       [ 11,   2,  23, ...,  80,  81,   5],\n",
       "       ...,\n",
       "       [ 65,   7,  66, ..., 125,   5, 126],\n",
       "       [  7,  66,  67, ...,   5, 126, 127],\n",
       "       [ 66,  67,  68, ..., 126, 127, 128]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## positional encoding \n",
    "\n",
    "### is going to hold the placement of next or before text also helps in grammer and contextual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(d_model)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = np.sin(pos * angle[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(pos * angle[:, 1::2])\n",
    "        self.angle = tf.cast(angle[np.newaxis, ...], dtype=np.float32)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
